{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bd151e70-c3c0-44c2-80ad-9cf8acc85e0c",
   "metadata": {},
   "source": [
    "This tutorial heavily draws on [https://pytorch.org/tutorials](https://pytorch.org/tutorials); refer to those pages for further detail."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2136dc53-3080-47a5-9995-ea2a1d57f8ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1d5ee6c-e12d-43e4-b489-fb00ae26f4c6",
   "metadata": {},
   "source": [
    "# Data types: everything is tensor\n",
    "\n",
    "Tensors in `torch` are the equivalent of `numpy` arrays. See [https://pytorch.org/docs/stable/torch.html](https://pytorch.org/docs/stable/torch.html) for detail.\n",
    "\n",
    "They can be created from python or numpy objects. The data type is automatically inferred, un less stated otherwise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5deb6c42-fb46-4006-af03-f9a8ce69c2a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# list\n",
    "data = [[0, 2],[5, 4]]\n",
    "\n",
    "# tensor from list\n",
    "t_data = torch.tensor(data)\n",
    "\n",
    "print('data:\\n{}\\nt_data:\\n{}'.format(data, t_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c2447ab-0c5a-42be-8429-a77841eed101",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('t_data shape: {}'.format(t_data.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c26ebfa-fd79-4b13-90f9-00de802b5b82",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('data type is {}, made of elements of type {}'.format(type(data), type(data[0][0])))\n",
    "print('t_data type is {}, made of elements of type {}'.format(type(t_data), t_data.dtype))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cae68bf6-05d9-4f03-9762-54c047383c3d",
   "metadata": {},
   "source": [
    "Types can be overridden, for example to save memory space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a56b0b6-3f49-4562-99e7-f9417cf37941",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tensor from list\n",
    "t_data = torch.tensor(data, dtype=torch.float32)\n",
    "\n",
    "print('t_data type is {}, made of elements of type {}'.format(type(t_data), t_data.dtype))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "690ab4f8-e84a-4b9e-9e5b-fae871c69895",
   "metadata": {},
   "source": [
    "Similarly, tensors can be built from `numpy` arrays."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2413d4a-170e-43e9-a98d-41c706072834",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_data = np.array(data, dtype=np.float32)\n",
    "\n",
    "t_data = torch.from_numpy(x_data)\n",
    "\n",
    "print('x_data type is {}, made of elements of type {}'.format(type(x_data), x_data.dtype))\n",
    "print('t_data type is {}, made of elements of type {}'.format(type(t_data), t_data.dtype))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6784233c-262c-463f-bbde-f0ae3b136d5e",
   "metadata": {},
   "source": [
    "Careful here: the `torch` tensor and `numpy` array are linked together (they share the same memory), so changing one changes the other."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffba4a35-3340-4749-98e4-2613326f444b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# add one to all elements\n",
    "t_data += 1\n",
    "print('t_data:\\n{}\\nx_data:\\n{}'.format(t_data, x_data))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e45e939c-932f-4bba-b266-151b0f1d3e15",
   "metadata": {},
   "source": [
    "But that is not the case with the list..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48708357-fee8-4634-b315-e70a1c03b532",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af83bd8e",
   "metadata": {},
   "source": [
    "Most usual constructors from `numpy` are available. See also `torch.zeros_like`, `torch.arange`, `torch.linspace`, `torch.eye`, etc.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf409a8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "shape = (2,3,)\n",
    "\n",
    "# tensor filled with zeros\n",
    "t_zeros = torch.zeros(shape)\n",
    "print('t_zeros: \\n {} \\n'.format(t_zeros))\n",
    "\n",
    "# tensor filled with ones\n",
    "t_ones = torch.ones(shape)\n",
    "print('t_ones: \\n {} \\n'.format(t_ones))\n",
    "\n",
    "# tensor filled with random variables\n",
    "t_rand = torch.rand(shape)\n",
    "print('t_rand: \\n {} \\n'.format(t_rand))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb96942b",
   "metadata": {},
   "source": [
    "Likewise, many functions from `numpy` are available as member functions, in particular linear algebra from `numpy.linalg`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "831e6945",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sum\n",
    "print('sum of t_rand: \\n{}'.format(t_rand.sum()))\n",
    "print('or equivalently')\n",
    "print('{} \\n'.format(torch.sum(t_rand)))\n",
    "      \n",
    "# mean\n",
    "print('mean of t_rand: \\n{}'.format(t_rand.mean(axis=1)))\n",
    "print('or equivalently')\n",
    "print('{} \\n'.format(torch.mean(t_rand, axis=1)))\n",
    "\n",
    "# std\n",
    "print('standard deviation of t_rand: \\n{}'.format(t_rand.std(axis=1)))\n",
    "print('or equivalently')\n",
    "print('{} \\n'.format(torch.std(t_rand, axis=1)))\n",
    "\n",
    "# svd\n",
    "print('singular value decomposition of t_rand: \\n{}'.format(t_rand.svd()))\n",
    "print('or equivalently')\n",
    "print('{} \\n'.format(torch.svd(t_rand)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e53137d1",
   "metadata": {},
   "source": [
    "## GPU vs CPU speed"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a419b5f-202a-49f2-ad93-43bca2b256f3",
   "metadata": {},
   "source": [
    "A particularity of `torch` is to keep track of the device where the object is stored (usually cpu or gpu)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "a10375d2-5e99-4403-9b13-f8a18e09b1f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "t_data is stored on: cuda:0\n"
     ]
    }
   ],
   "source": [
    "print('t_data is stored on: {}'.format(t_data.device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "972f1155",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "t_data is stored on: cuda:0\n",
      "t_data is stored on: cpu\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "\n",
    "t_data = t_data.to(device)\n",
    "print('t_data is stored on: {}'.format(t_data.device))\n",
    "\n",
    "t_data = t_data.to('cpu')\n",
    "print('t_data is stored on: {}'.format(t_data.device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38e4d70a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "N=5000\n",
    "\n",
    "# Matrix multiplication on CPU\n",
    "\n",
    "A = torch.rand(N, N, device='cpu')\n",
    "B = torch.rand(N, N, device='cpu')\n",
    "\n",
    "start = time.time()\n",
    "result = A @ B\n",
    "end = time.time()\n",
    "\n",
    "print('CPU time :', end - start)\n",
    "\n",
    "\n",
    "# Matrix multiplication on GPU\n",
    "\n",
    "A = A.cuda()\n",
    "B = B.cuda()\n",
    "\n",
    "\n",
    "start = time.time()\n",
    "result = A @ B\n",
    "torch.cuda.synchronize()\n",
    "end = time.time()\n",
    "\n",
    "print('GPU time :', end - start)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6db4957e",
   "metadata": {},
   "source": [
    "# Datasets and loaders\n",
    "\n",
    "Data manipulation are eased in pytorch by functions that can load big datasets and select batches of samples with randomization. Transformation are also used to normalize the data (here the contrast to a given range). Many datasets are available, like images with `torchvision`.\n",
    "This tutorial heavily draws [https://pytorch.org/tutorials/beginner/nn_tutorial.html](https://pytorch.org/tutorials/beginner/nn_tutorial.html)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fee7df60-f5ca-4aa6-bcdc-7cf4394c89b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import datasets\n",
    "from torchvision.transforms import ToTensor, Lambda\n",
    "from torch.utils.data import DataLoader\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7e1a733-ca91-4eb3-8a42-4a422eaed108",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = datasets.MNIST(\n",
    "    root='./tmp',\n",
    "    train=True,\n",
    "    download=True,\n",
    "    transform=ToTensor()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c210034a",
   "metadata": {},
   "outputs": [],
   "source": [
    "image, label = train_data[0]\n",
    "\n",
    "print(image.shape)\n",
    "print(label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f6db233-d3db-464a-be45-963b06528564",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot random example samples\n",
    "figure = plt.figure(figsize=(8, 8))\n",
    "cols, rows = 3, 3\n",
    "for i in range(1, cols * rows + 1):\n",
    "    sample_idx = torch.randint(len(train_data), size=(1,)).item()\n",
    "    img, label = train_data[sample_idx]\n",
    "    figure.add_subplot(rows, cols, i)\n",
    "    plt.title(label)\n",
    "    plt.axis('off')\n",
    "    plt.imshow(img.squeeze(), cmap='gray')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91bfa9a2-e189-4d01-b77f-456b0d1d4e9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# make a batch loader\n",
    "train_dataloader = DataLoader(train_data, batch_size=64, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e250382-9e9d-4e2c-b2aa-44602dda2404",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load new batch\n",
    "train_features, train_labels = next(iter(train_dataloader))\n",
    "print('Feature batch shape: {}'.format(train_features.size()))\n",
    "print('Labels batch shape: {}'.format(train_labels.size()))\n",
    "\n",
    "# plot first sample of batch\n",
    "plt.figure()\n",
    "plt.title(train_labels[0].numpy())\n",
    "plt.axis('off')\n",
    "plt.imshow(train_features[0].squeeze(), cmap='gray')\n",
    "plt.colorbar()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f315114-5d10-48dc-a4b1-5ad7fcea4c7c",
   "metadata": {},
   "source": [
    "Depending on the neural network design, we might need to format the labels (integer originally) into vectors, with 0s everywhere except for a 1 at the index of the corresponding class (one-hot encoding)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbab091f-e409-448d-b211-96a26390dff8",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = datasets.MNIST(\n",
    "    root='tmp',\n",
    "    train=False,\n",
    "    download=True,\n",
    "    transform=ToTensor(),\n",
    "    target_transform=Lambda(lambda y: torch.zeros(10, dtype=torch.float).scatter_(0, torch.tensor(y), value=1))\n",
    ")\n",
    "\n",
    "X, y = train_data[0]\n",
    "\n",
    "# plot first sample of batch\n",
    "plt.figure()\n",
    "plt.axis('off')\n",
    "plt.imshow(X.squeeze(), cmap='gray')\n",
    "plt.colorbar()\n",
    "plt.show()\n",
    "\n",
    "# plot the one-hot encoded label vector\n",
    "plt.figure()\n",
    "plt.imshow(y.reshape([1,10]), cmap='gray')\n",
    "plt.yticks([])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e4efc01",
   "metadata": {},
   "source": [
    "# Gradient and parameter update"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28bc0e71",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "\n",
    "# number of samples\n",
    "n = 5\n",
    "\n",
    "# ground truth: y is a linear function of x contaminated with noise\n",
    "x = torch.arange(n, dtype=torch.float)  # input tensor\n",
    "print(f'input: {x}')\n",
    "a_true = 0.5\n",
    "b_true = 1.5\n",
    "\n",
    "y = a_true * x + b_true + torch.randn(n)  # expected/desired output\n",
    "\n",
    "plt.figure()\n",
    "plt.scatter(x, y, c='b', label='data')\n",
    "plt.plot(x, a_true*x+b_true, '--k', label='th')\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('y')\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "# we start from a random \n",
    "a = torch.randn(1, requires_grad=True)\n",
    "b = torch.randn(1, requires_grad=True)\n",
    "z = a * x + b\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea2c17a7",
   "metadata": {},
   "source": [
    "\n",
    "We calculate the loss of our linear regression, which is the mean sqarred error here.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0b19ddf",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = torch.nn.functional.mse_loss(z, y)\n",
    "print('x:', x)\n",
    "print('y:', y)\n",
    "print('a:', a)\n",
    "print('b:', b)\n",
    "loss.backward()\n",
    "print('gradient for a:', a.grad)\n",
    "print('gradient for b:', b.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a5f23e2",
   "metadata": {},
   "source": [
    "\n",
    "Let's try to run the previous cell a second time...\n",
    "\n",
    "To update the parameters, one has to first calculate the gradient, then modify the parameters with it, but the gradient has to be stopped to do the modification. Otherwise, the parameters (that influence the loss) will be linked to and thus depend on the loss for their calculation, which makes circular dependencies.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9568edf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# learning rate\n",
    "eta = 0.1\n",
    "\n",
    "for i in range(10):\n",
    "    print('a:', a)\n",
    "    print('b:', b)\n",
    "    # calculate output\n",
    "    z = a * x + b\n",
    "    loss = torch.nn.functional.mse_loss(z, y)\n",
    "    loss.backward()\n",
    "    print('gradient for a:', a.grad)\n",
    "    print('gradient for b:', b.grad)\n",
    "    # update the parameters with the \n",
    "    with torch.no_grad():\n",
    "        a.copy_(a - eta * a.grad)\n",
    "        b.copy_(b - eta * b.grad)\n",
    "    # reset the gradients\n",
    "    a.grad.data.zero_()\n",
    "    b.grad.data.zero_()\n",
    "a_est = a.detach()\n",
    "b_est = b.detach()\n",
    "\n",
    "plt.figure()\n",
    "plt.scatter(x, y, c='b', label='data')\n",
    "plt.plot(x, a_true*x+b_true, '--k', label='th')\n",
    "plt.plot(x, a_est*x+b_est, '--r', label='est')\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('y')\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a0ee472",
   "metadata": {},
   "source": [
    "In `torch`, one can disable the gradient calculation using `torch.no_grad()`. Moreover, `detach` can be used to retrieve the trained parameters and use them e.g. in `numpy` functions (as with plotting above).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a4cc919",
   "metadata": {},
   "outputs": [],
   "source": [
    "z = a * x + b\n",
    "print(z.requires_grad)\n",
    "\n",
    "with torch.no_grad():\n",
    "    z = a * x + b\n",
    "print(z.requires_grad)\n",
    "z = a * x + b\n",
    "z_det = z.detach()\n",
    "print(z.requires_grad)\n",
    "print(z_det.requires_grad)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "380a37c2",
   "metadata": {},
   "source": [
    "\n",
    "# Model and parameter optimization\n",
    "\n",
    "The above mechanism can be automatized after defining the linear regression as a neural network model (based `torch.nn` and with the same trainable parameters as above) and using an optimizer.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2170185",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from torch import nn, optim\n",
    "\n",
    "class LinReg(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.lin = nn.Linear(1, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # calculate output\n",
    "        y = self.lin(x)\n",
    "        return y.reshape(x.shape[0])\n",
    "    \n",
    "# create an instanciation of the linear regression model\n",
    "model = LinReg()\n",
    "\n",
    "# loss function\n",
    "loss_fn = nn.MSELoss()\n",
    "\n",
    "# optimizer\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b06b342a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# optimization loop\n",
    "for i in range(50):\n",
    "    # compute prediction and loss\n",
    "    pred = model(x.reshape([5,1]))\n",
    "    loss = loss_fn(pred, y)\n",
    "\n",
    "    # backpropagation of loss error\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    # report loss\n",
    "    print('epoch {}, loss = {}'.format( i, loss.detach()))\n",
    "[print(p) for p in model.parameters()]\n",
    "a_est2, b_est2 = model.parameters()\n",
    "a_est2 = a_est2.detach().flatten()\n",
    "b_est2 = b_est2.detach().flatten()\n",
    "\n",
    "plt.figure()\n",
    "plt.scatter(x, y, c='b', label='data')\n",
    "plt.plot(x, a_true*x+b_true, '--k', label='th')\n",
    "plt.plot(x, a_est2*x+b_est2, '--r', label='est')\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('y')\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
