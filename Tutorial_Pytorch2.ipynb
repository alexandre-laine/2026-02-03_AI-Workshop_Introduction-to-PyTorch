{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd3e1abe",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import torch\n",
    "from torch import nn, optim\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets\n",
    "from torchvision.transforms import ToTensor\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c89096f",
   "metadata": {},
   "source": [
    "# Neural network implementation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89df698f",
   "metadata": {},
   "source": [
    "\n",
    "The following code creates a neural network made of several layers, `Linear` for matrix multiplication by some (trainable) weights, `ReLu` for recifying units (threshold linear). The generic class for neural network is `nn.Module`, which imposes some constraints on the design, but all of this is kind of hidden to the user.\n",
    "\n",
    "Note that for running on gpu, functions are provided to move the model and data (see argument `device` for and `model = NeuralNetwork().to(device)`).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbb8ab71",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.linear_relu_stack = nn.Sequential(\n",
    "            nn.Linear(28*28, 100),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(100, 10),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.flatten(x)\n",
    "        logits = self.linear_relu_stack(x)\n",
    "        return logits\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03a41b0e",
   "metadata": {},
   "source": [
    "To calculate the output for a given input (image), we use the `forward` method, or simply `model(X)`. The predicted class corresponds to the maximum output (the `Softmax` simply converts the output to probabilities). \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e659814",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create an instance of the model\n",
    "model = MLP()\n",
    "\n",
    "# create a random input and apply it\n",
    "X = torch.rand(1, 28, 28)\n",
    "logits = model.forward(X)\n",
    "print('output:\\n', logits)\n",
    "model(X)\n",
    "pred_probab = nn.Softmax(dim=1)(logits)\n",
    "print('probabilities:\\n', pred_probab)\n",
    "y_pred = pred_probab.argmax(1)\n",
    "print('predicted class:', y_pred)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "222b4002",
   "metadata": {},
   "source": [
    "Now we can check the parameters of the model, in particular to know which ones will be trained.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d67b9057",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('model structure:\\n', model)\n",
    "for name, param in model.named_parameters():\n",
    "    print('layer: {} | size: {} \\nweight values : {} \\n'.format(name, param.size(), param.ravel()[:100]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2ca21a6",
   "metadata": {},
   "source": [
    "# Training using autograd\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef6448d3",
   "metadata": {},
   "source": [
    "\n",
    "Now we can present the model with train data and optimize the weights to reduce a loss function.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "946767bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# loss function\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "# optimizer with learning rate lr\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.01)\n",
    "def train_loop(dataloader, model, loss_fn, optimizer):\n",
    "    size = len(dataloader.dataset)\n",
    "    batch_size = dataloader.batch_size\n",
    "    # set the model to training mode (good practice even if not crucial here)\n",
    "    model.train()\n",
    "    for batch, (X, y) in enumerate(dataloader):\n",
    "        # compute prediction and loss\n",
    "        pred = model(X)\n",
    "        loss = loss_fn(pred, y)\n",
    "\n",
    "        # backpropagation\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        if batch % 50 == 0:\n",
    "            loss, current = loss.item(), batch * batch_size + len(X)\n",
    "            print('loss: {:>7f}  [{:>5d}/{:>5d}]'.format(loss, current, size))\n",
    "\n",
    "\n",
    "def test_loop(dataloader, model, loss_fn):\n",
    "    # set the model to evaluation mode (good practice even if not crucial here)\n",
    "    model.eval()\n",
    "    size = len(dataloader.dataset)\n",
    "    num_batches = len(dataloader)\n",
    "    test_loss, correct = 0, 0\n",
    "\n",
    "    # evaluate the model with torch.no_grad() to ensure that no gradients are computed during test mode\n",
    "    with torch.no_grad():\n",
    "        for X, y in dataloader:\n",
    "            pred = model(X)\n",
    "            test_loss += loss_fn(pred, y).item()\n",
    "            correct += (pred.argmax(1) == y).type(torch.float).sum().item()\n",
    "\n",
    "    test_loss /= num_batches\n",
    "    correct /= size\n",
    "    print('test evaluation: \\n accuracy: {:>0.1f}%, avg loss: {:>8f} \\n'.format(100*correct, test_loss))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5322d7f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data = datasets.MNIST(\n",
    "    root='./tmp',\n",
    "    train=True,\n",
    "    download=True,\n",
    "    transform=ToTensor()\n",
    ")\n",
    "\n",
    "test_data = datasets.MNIST(\n",
    "    root='./tmp',\n",
    "    train=False,\n",
    "    download=True,\n",
    "    transform=ToTensor()\n",
    ")\n",
    "\n",
    "# batch size\n",
    "batch_size = 200\n",
    "\n",
    "train_dataloader = DataLoader(training_data, batch_size=batch_size)\n",
    "test_dataloader = DataLoader(test_data, batch_size=batch_size)\n",
    "# training epochs (go through full training set per epoch)\n",
    "epochs = 3\n",
    "\n",
    "for t in range(epochs):\n",
    "    print('epoch {}\\n-------------------------------'.format(t+1))\n",
    "    train_loop(train_dataloader, model, loss_fn, optimizer)\n",
    "    test_loop(test_dataloader, model, loss_fn)\n",
    "    \n",
    "print('finished')\n",
    "# Initialization and optimization schemes\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84790463",
   "metadata": {},
   "source": [
    "\n",
    "We can now look into some more fine tuning of the model before and during its optimization. For instance, what is implied in using `nn.Module` is the initialization of the weights.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7cdc9e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set a seed for the random number generator\n",
    "torch.manual_seed(12345)\n",
    "\n",
    "# check that the generated network has always the same initial weights\n",
    "model = MLP()\n",
    "for name, param in model.named_parameters():\n",
    "    print('layer: {} | size: {} \\nweight values : {} \\n'.format(name, param.size(), param.ravel()[:10]))\n",
    "# loss function\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "# optimizer with learning rate lr\n",
    "optimizer = optim.Adam(model.parameters())\n",
    "\n",
    "# loop over epochs\n",
    "for t in range(epochs):\n",
    "    print('epoch {}\\n-------------------------------'.format(t+1))\n",
    "    train_loop(train_dataloader, model, loss_fn, optimizer)\n",
    "    test_loop(test_dataloader, model, loss_fn)\n",
    "    \n",
    "print('finished')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d3611b3",
   "metadata": {},
   "source": [
    "Adam is an optimizer with adaptive learning rate that has been shown to reach better performance than classical stochastic gradient descent. Check [https://pytorch.org/docs/stable/optim.html](https://pytorch.org/docs/stable/optim.html) for other powerful options.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ebd3743",
   "metadata": {},
   "source": [
    "# Saving a trained model\n",
    "\n",
    "High-level functions are provided by `torch` to save models defined using `torch.nn`. This makes it easy to record the evolution of trained parameters, etc. One can save the parameters only with ̀̀`state_dict` as below (but then one needs to know the architecture, i.e. in which layer the parameters go...) or the full model architecture with `torch.save` directly.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cae0687c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save all relevant model parameters\n",
    "torch.save(model.state_dict(), './tmp/mlp_weights.pth')\n",
    "# random initialization\n",
    "model = MLP()\n",
    "model.eval()\n",
    "test_loop(test_dataloader, model, loss_fn)\n",
    "\n",
    "# load previously trained weights\n",
    "model.load_state_dict(torch.load('./tmp/mlp_weights.pth'))\n",
    "model.eval()\n",
    "test_loop(test_dataloader, model, loss_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b0796ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9db3678c",
   "metadata": {},
   "source": [
    "Adapted from https://pytorch.org/tutorials/beginner/blitz/cifar10_tutorial.html\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcc61467",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Check Data\n",
    "transform = transforms.Compose(\n",
    "    [transforms.ToTensor(),\n",
    "     transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
    "\n",
    "batch_size = 4\n",
    "\n",
    "trainset = torchvision.datasets.CIFAR10(root='./tmp', train=True,\n",
    "                                        download=True, transform=transform)\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=batch_size,\n",
    "                                          shuffle=True, num_workers=2)\n",
    "\n",
    "testset = torchvision.datasets.CIFAR10(root='./tmp', train=False,\n",
    "                                       download=True, transform=transform)\n",
    "testloader = torch.utils.data.DataLoader(testset, batch_size=batch_size,\n",
    "                                         shuffle=False, num_workers=2)\n",
    "\n",
    "classes = ('plane', 'car', 'bird', 'cat',\n",
    "           'deer', 'dog', 'frog', 'horse', 'ship', 'truck')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e791ecc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# functions to show an image\n",
    "\n",
    "\n",
    "def imshow(img):\n",
    "    img = img / 2 + 0.5     # unnormalize\n",
    "    npimg = img.numpy()\n",
    "    plt.imshow(np.transpose(npimg, (1, 2, 0)))\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "# get some random training images\n",
    "dataiter = iter(trainloader)\n",
    "images, labels = next(dataiter)\n",
    "\n",
    "# show images\n",
    "imshow(torchvision.utils.make_grid(images))\n",
    "# print labels\n",
    "print(' '.join(f'{classes[labels[j]]:5s}' for j in range(batch_size)))\n",
    "# get some random training images\n",
    "dataiter = iter(trainloader)\n",
    "images, labels = next(dataiter)\n",
    "x = images[0] / 2 + 0.5\n",
    "print(images.shape)\n",
    "print(x.shape)\n",
    "\n",
    "\n",
    "plt.figure(figsize=(16,4))\n",
    "plt.subplot(1,4,1)\n",
    "plt.imshow(np.transpose(x.numpy(), (1, 2, 0)))\n",
    "for i in range(3):\n",
    "    plt.subplot(1,4,i+2)\n",
    "    plt.imshow(x[i,:,:].numpy(), cmap='Greys')\n",
    "## Test Image Transformations\n",
    "plt.imshow(x[1,:,:].numpy(), cmap='Greys', interpolation='nearest')\n",
    "plt.savefig('ex_img')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52c06b03",
   "metadata": {},
   "outputs": [],
   "source": [
    "conv = nn.Conv2d(3, 6, 5)\n",
    "\n",
    "conv(x).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4acf4181",
   "metadata": {},
   "outputs": [],
   "source": [
    "pool = nn.MaxPool2d(2, 2)\n",
    "\n",
    "pool(conv(x)).shape\n",
    "torch.flatten(pool(conv(x)), 0).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ff62160",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Build and Run NEural Network Model\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# convolutional neural network\n",
    "class CNN_Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 6, 5)\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        self.conv2 = nn.Conv2d(6, 16, 5)\n",
    "        self.fc1 = nn.Linear(16 * 5 * 5, 120)\n",
    "        self.fc2 = nn.Linear(120, 84)\n",
    "        self.fc3 = nn.Linear(84, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(F.relu(self.conv1(x)))\n",
    "        x = self.pool(F.relu(self.conv2(x)))\n",
    "        x = torch.flatten(x, 1) # flatten all dimensions except batch\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "model = CNN_Net()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "027e1f19",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.001, momentum=0.9)\n",
    "for epoch in range(2):  # loop over the train data multiple times\n",
    "\n",
    "    running_loss = 0.0\n",
    "    for i, data in enumerate(trainloader, 0):\n",
    "        # get the inputs; data is a list of [inputs, labels]\n",
    "        inputs, labels = data\n",
    "\n",
    "        # zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # forward + backward + optimize\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # print statistics\n",
    "        running_loss += loss.item()\n",
    "        if i % 2000 == 0:    # print every 2000 mini-batches\n",
    "            print(f'[{epoch + 1}, {i + 1:5d}] loss: {running_loss / 2000:.3f}')\n",
    "            running_loss = 0.0\n",
    "\n",
    "print('Finished Training')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4a5fc4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_acc(dataloader):\n",
    "    \n",
    "    with torch.no_grad():\n",
    "\n",
    "        # number of correct predictions\n",
    "        corr_pred = 0\n",
    "        # number of samples\n",
    "        n_samples = 0\n",
    "        \n",
    "        # loop over the test data\n",
    "        for i, data in enumerate(dataloader, 0):\n",
    "            # get the inputs; data is a list of [inputs, labels]\n",
    "            inputs, labels = data\n",
    "    \n",
    "            # forward only and prediction\n",
    "            outputs = model(inputs)\n",
    "            \n",
    "            # accuracy as count of correct prediction and total number of samples\n",
    "            corr_pred += (torch.argmax(outputs, axis=1) == labels).sum()\n",
    "            n_samples += outputs.shape[0]\n",
    "\n",
    "        # return accuracy\n",
    "        return corr_pred / n_samples\n",
    "\n",
    "        \n",
    "print('Accuracy on train set:', calc_acc(trainloader))\n",
    "print('Accuracy on test set:', calc_acc(testloader))\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
